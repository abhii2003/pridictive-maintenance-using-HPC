{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import time\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard available at: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start Dask client\n",
    "cluster = LocalCluster(n_workers=4,threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# ðŸ“Œ Load CSV Data (Replaces Synthetic Data)\n",
    "file_path = \"ai4i2020.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns (UDI and Product ID are unique identifiers)\n",
    "df.drop(columns=[\"UDI\", \"Product ID\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to simpler names for ease of use\n",
    "df.rename(columns={\n",
    "    \"Air temperature [K]\": \"air_temp\",\n",
    "    \"Process temperature [K]\": \"process_temp\",\n",
    "    \"Rotational speed [rpm]\": \"rpm\",\n",
    "    \"Torque [Nm]\": \"torque\",\n",
    "    \"Tool wear [min]\": \"tool_wear\",\n",
    "    \"Machine failure\": \"failure_risk\"\n",
    "}, inplace=True)\n",
    "# Convert to Dask DataFrame\n",
    "ddf = dd.from_pandas(df, npartitions=8)\n",
    "\n",
    "# Step 4: Preprocess Data\n",
    "def preprocess_data(df):\n",
    "    df['temp_diff'] = df['process_temp'] - df['air_temp']  # Process Temp vs Air Temp\n",
    "    df['torque_rpm_ratio'] = df['torque'] / df['rpm']  # Torque-to-RPM ratio\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Dask Preprocessing Time: 0.003 seconds\n",
      "Dask Preprocessing Time: 2.484 seconds\n"
     ]
    }
   ],
   "source": [
    "# Non-Dask Preprocessing\n",
    "start_time = time.time()\n",
    "processed_data_non_dask = preprocess_data(df)\n",
    "end_time = time.time()\n",
    "non_dask_time = end_time - start_time\n",
    "print(f\"Non-Dask Preprocessing Time: {non_dask_time:.3f} seconds\")\n",
    "\n",
    "# Dask Preprocessing\n",
    "start_time = time.time()\n",
    "ddf_processed = ddf.map_partitions(preprocess_data)\n",
    "ddf_processed = ddf_processed.persist()\n",
    "end_time = time.time()\n",
    "dask_time = end_time - start_time\n",
    "print(f\"Dask Preprocessing Time: {dask_time:.3f} seconds\")\n",
    "# Prepare Features and Scale\n",
    "features = [\"air_temp\", \"process_temp\", \"rpm\", \"torque\", \"tool_wear\", \"temp_diff\", \"torque_rpm_ratio\"]\n",
    "target = \"failure_risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91936\\Desktop\\PC\\.venv\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91936\\Desktop\\PC\\.venv\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Dask Scaling Time: 25.116 seconds\n",
      "Dask Scaling Time: 0.087 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91936\\Desktop\\PC\\.venv\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91936\\Desktop\\PC\\.venv\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Non-Dask Scaling\n",
    "start_time = time.time()\n",
    "X_non_dask = processed_data_non_dask[features]\n",
    "y_non_dask = processed_data_non_dask[target]\n",
    "scaler_non_dask = StandardScaler()\n",
    "X_scaled_non_dask = scaler_non_dask.fit_transform(X_non_dask)\n",
    "end_time = time.time()\n",
    "non_dask_scaling_time = end_time - start_time\n",
    "print(f\"Non-Dask Scaling Time: {non_dask_scaling_time:.3f} seconds\")\n",
    "\n",
    "# Dask Scaling\n",
    "start_time = time.time()\n",
    "X_dask = ddf_processed[features]\n",
    "y_dask = ddf_processed[target]\n",
    "scaler_dask = StandardScaler()\n",
    "X_scaled_dask = scaler_dask.fit_transform(X_dask)\n",
    "X_scaled_dask = X_scaled_dask.persist()  \n",
    "end_time = time.time()\n",
    "dask_scaling_time = end_time - start_time\n",
    "print(f\"Dask Scaling Time: {dask_scaling_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Dask Model Training Time: 108.112 seconds\n",
      "Dask Model Training Time: 34.898 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model = ParallelPostFit(estimator=rf)\n",
    "\n",
    "# Non-Dask Model Training\n",
    "start_time = time.time()\n",
    "X_train_non_dask, X_test_non_dask, y_train_non_dask, y_test_non_dask = train_test_split(\n",
    "    X_scaled_non_dask, y_non_dask, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "model.fit(X_train_non_dask, y_train_non_dask)\n",
    "end_time = time.time()\n",
    "non_dask_training_time = end_time - start_time\n",
    "print(f\"Non-Dask Model Training Time: {non_dask_training_time:.3f} seconds\")\n",
    "\n",
    "# Dask Model Training\n",
    "start_time = time.time()\n",
    "X_train_dask, X_test_dask, y_train_dask, y_test_dask = train_test_split(\n",
    "    X_scaled_dask, y_dask, test_size=0.2, random_state=42,shuffle=True\n",
    ")\n",
    "with ProgressBar():\n",
    "    model.fit(X_train_dask.compute(), y_train_dask.compute())\n",
    "end_time = time.time()\n",
    "dask_training_time = end_time - start_time\n",
    "print(f\"Dask Model Training Time: {dask_training_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Dask Prediction Time: 7.549 seconds\n",
      "Dask Prediction Time: 0.051 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "# Non-Dask Prediction\n",
    "start_time = time.time()\n",
    "y_pred_non_dask = model.predict(X_test_non_dask)\n",
    "end_time = time.time()\n",
    "non_dask_prediction_time = end_time - start_time\n",
    "print(f\"Non-Dask Prediction Time: {non_dask_prediction_time:.3f} seconds\")\n",
    "\n",
    "# Dask Prediction\n",
    "start_time = time.time()\n",
    "y_pred_dask = model.predict(X_test_dask.compute())\n",
    "end_time = time.time()\n",
    "dask_prediction_time = end_time - start_time\n",
    "print(f\"Dask Prediction Time: {dask_prediction_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 21:18:00,114 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-03 21:18:00,116 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-03 21:18:00,119 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-03 21:18:00,120 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time without parallel processing :  140.77893042564392\n",
      "The total time with parallel processing :  37.52039980888367\n"
     ]
    }
   ],
   "source": [
    "total_no_dask_time =  non_dask_time + non_dask_scaling_time + non_dask_training_time + non_dask_prediction_time\n",
    "total_dask_time = dask_time + dask_scaling_time + dask_training_time + dask_prediction_time\n",
    "\n",
    "print(\"The total time without parallel processing : \",total_no_dask_time)\n",
    "print(\"The total time with parallel processing : \",total_dask_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
